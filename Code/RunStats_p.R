
################################################################################
# BOOTSTRAP ANALYSIS FOR NEPAL GRASS/RHINO PROJECT
################################################################################

#############
#HHakkinen Jan 2026
#DESCRIPTION
#In the following file we will:
#load and plot samples
#load tiles of 0/1 mikania presences and produce summary statistics per location and management
#compile and run bootstrap analysis
#condense and then plot overall results and representative model

#input files:
#fcStats.shp - shape files of each site in Nepal, includes information on management, grass presence and rhino presence
#Mikania_Binary_03m-0000000000-xxxxxxxxxx.tif - tiled output rasters from GEE of estimated Mikania presence/absence
#samplePoints_xx-xx.csv - sample points generated by GEE

#outputs:
# summary statistics of mikania presence per location and management type
# diagnostics - residual plots per random bootstrap
# model output - summarised output per bootstrap
# ParRawFig and ParTransFig - plotted summaries of parameter estimates
#############


#clear env
rm(list=ls())

#setwd("C:/Users/henry.hakkinen/OneDrive - Zoological Society of London/Documents/Research/ZSL/Nepal/bootstrapSamples")
setwd("C:/Users/Henry/Documents/Research/ZSL/Nepal/bootstrapSamples")

library(terra)
library(sf)
library(leaflet)
library(lme4)
library(boot)
library(DHARMa)
library(ggplot2)
library(zoo)
library(dplyr)
library(ggpubr)


#file assumes raster files are in a folder named "rasters"
#file assumes bootstrap csvs are in a folder named "bootstrapSamples"

#make sure output folders exist
dir.create(file.path("../diagnostics"), showWarnings = F)
dir.create(file.path("../modelOutput"), showWarnings = F)
dir.create(file.path("../rasters/temp"), showWarnings = F)


#### COMPILE AND NEATEN SHAPEFILE AND RASTER DATA ######################################

### MAKE MAPS
#make a map so we can see the data in context

sh1<-st_read("fcStats.shp")

table(sh1$geo_type)
#sh1<-sh1[sh1$geo_type=="GeometryCollection" | sh1$geo_type=="MultiPolygon",]

#separate collections
#sh2<-st_cast(sh1, "POLYGON")
#table(sh2$geo_type)
#create the base map
p<- leaflet() %>% 
  setView(lng = 84.3, lat = 27.5, zoom = 11)  %>% #setting the view over ~ center of North America
  addTiles()

p <- p %>%
  addPolygons(data=sh1, color = "#444444", weight = 1, smoothFactor = 0.5,
              opacity = 1.0, fillOpacity = 0.5,
              fillColor = ~colorQuantile("YlOrRd", fid)(fid),
              highlightOptions = highlightOptions(color = "white", weight = 2,
                                                  bringToFront = TRUE),
              popup=paste0("ID:", sh1$fid, "</br>mean: ", sh1$mean, 
                           "</br>count: ", sh1$count, "</br>rhino: ", sh1$Rhino,
                           "</br>location: ", sh1$Location))
#label = ~as.character(Site))
p

### LOAD RASTER DATA ON SITES

rlist<-list.files("../rasters/", "tif$", full.names = TRUE)

#do quick transformation to align CRS
r<-rlist[1]
sh1t<-st_transform(sh1, crs(rast(paste0("../rasters/", r))))
#plot(sh1t$geometry, axes=T)

#r<-rlist[2]

#load and process each tile
#note: SLOW, takes 20-30 min per tile
for(r in rlist[1:28]){
  
  a<-Sys.time()
  
  print(which(r==rlist))
  
  #load raster, crop, trim and mask in most efficient order
  r1<-rast(r)
  r1crop<-crop(r1, sh1t)
  r1crops <- trim(r1crop)
  r1mask<-mask(r1crops, sh1t, touches=F)

  cat("Finished crop and mask: ", capture.output(Sys.time()-a ), "\n")
  
  #if there are no overlaps then skip next stage

  if(global(r1mask, "notNA")>0){
    
    #plots to check process
    #plot(r1); plot(sh1t$geometry, add=T)
    # plot(r1crops); plot(sh1t$geometry, add=T)
    # plot(r1mask); plot(sh1t$geometry, add=T)
    
    #these are large rasters, I found this the fastest process
    #rasterize info to match our presence/absence raster
    #fidinfo<-rasterize(sh1t, r1mask, field=c("fid"), touches=T)
    Mgmtinfo<-rasterize(sh1t, r1mask, field=c("Mgmt2022"), touches=T)
    Locinfo<-rasterize(sh1t, r1mask, field=c("Location"), touches=T)
  
    cat("Finished rasterize: ", capture.output(Sys.time()-a ), "\n")
  
    #count how many presences and absences per management regime and per site (all data)
    finalLay<-c(r1mask, Mgmtinfo, Locinfo)
  
  
    #count how many presences and absences per management regime and per site (valid data only) 
    #creates unique combinations we can count and process later
    u <- unique(finalLay, as.raster=TRUE)
    freqt<-freq(u)
    
    write.csv(freqt, paste0("../rasters/temp/Tile",which(r==rlist), ".csv"))
  }

  cat("Finished tile: ", capture.output(Sys.time()-a ))
  cat("\n\n")
}

#once that's finished
#load and compile temp files from each
tilelist<-list.files("../rasters/temp/", ".csv", full.names = T)

tile1<-read.csv(tilelist[1])
tile1$Tile<-1

for(tlump in 2:length(tilelist)){
  
  ttemp<-read.csv(tilelist[tlump])
  ttemp$Tile<-tlump
  
  tile1<-rbind(tile1, ttemp)
  
}

#neaten lump
tile1$X<-NULL; tile1$layer<-NULL

#used unique strings to track cell counts
#for each, split and make into individual fields
valspl<-strsplit(tile1$value, split="_")
tile1$classification<-unlist(lapply(valspl, `[[`, 1))
tile1$Mgmt2022<-unlist(lapply(valspl, `[[`, 2))
tile1$Location<-unlist(lapply(valspl, `[[`, 3))

#remove whitespace
tile1$classification<-gsub(" ", "", tile1$classification)

#cannot have invalid classification, remove
tile1<-tile1[tile1$classification!="NA",]

#two types of mowing category, merge
tile1$Mgmt2022[tile1$Mgmt2022=="Mowing"]<-"Mowed"
tile1$Mgmt2022[tile1$Mgmt2022=="Mowing and burning"]<-"Burned and mowed"

All<-c("All", "All", sum(tile1$count[tile1$classification=="1"]), sum(tile1$count[tile1$classification=="0"]))

Chi<-c("Chitwan", "All", sum(tile1$count[tile1$Location=="Chitwan"& tile1$classification=="1"]),
           sum(tile1$count[tile1$Location=="Chitwan"& tile1$classification=="0"]))

Par<-c("Parsa", "All", sum(tile1$count[tile1$Location=="Parsa"& tile1$classification=="1"]),
       sum(tile1$count[tile1$Location=="Parsa"& tile1$classification=="0"]))

summarydf<-rbind(All, Chi, Par)

#go through management approaches
for (man in unique(tile1$Mgmt2022)){
  print(man)
  
  temp<-c("All", man, sum(tile1$count[tile1$Mgmt2022==man& tile1$classification=="1"]),
         sum(tile1$count[tile1$Mgmt2022==man& tile1$classification=="0"]))
  summarydf<-rbind(summarydf, temp)
}


#go through management approaches and locations
for (man in unique(tile1$Mgmt2022)){

  subf<-tile1[tile1$Mgmt2022==man,]
  
  for(loc in unique(subf$Location)){
    temp<-c(loc, man, 
            sum(subf$count[subf$Mgmt2022==man & 
                          subf$Location==loc &
                          subf$classification=="1"]),
            sum(subf$count[subf$Mgmt2022==man &
                            subf$Location==loc &
                           subf$classification=="0"]))
    summarydf<-rbind(summarydf, temp)
  }
}

#neaten and add proportion
summarydf<-as.data.frame(summarydf)
names(summarydf)<-c("Location", "Mgmt2022", "NPres", "NAbs")
summarydf$NPres<-as.numeric(summarydf$NPres)
summarydf$NAbs<-as.numeric(summarydf$NAbs)
summarydf$propPres<-round(summarydf$NPres/(summarydf$NPres+summarydf$NAbs), 3)

#write out summary for later use
write.csv(summarydf, "../summary.csv", row.names = F)   

#note that the site with burning, uprooting and mowing is removed from analysis, 
#as it is not stable in bootstrapping (due to relative small sample)


#### COMPILE AND NEATEN BOOTSTRAP DATA ######################################
#read and bind all outputs from sampling
df1<-read.csv(list.files(".", ".csv")[1])

for(i in 2:length(list.files(".", ".csv"))){
  print(i)
  dd<-read.csv(list.files(".", ".csv")[i])
  df1<-rbind(df1, dd)
}

#The basic steps
#run a binomial regression for every dataset
#check the tests actually match residuals etc.
#store parameter estimates from each one
#check they converge as time goes on

#get output parameter estimates


#https://ianwhitestone.work/how-many-bootstrap-samples/

#don't need coordinates anymore, leave out
# df1<-as.data.frame(sh1)
# df1$geometry<-NULL
# df1$lat<-st_coordinates(sh1)[,"Y"]
# df1$lon<-st_coordinates(sh1)[,"X"]

df1$.geo<-NULL

#Change levels to text
df1$Location[df1$Location==1]<-"Chitwan"
df1$Location[df1$Location==2]<-"Parsa"

df1$Mgmt2016[df1$Mgmt2016==1]<-NA
df1$Mgmt2016[df1$Mgmt2016==2]<-"DryingWetland" #Drying up wetland
df1$Mgmt2016[df1$Mgmt2016==3]<-"Existing" #Existed from paste
df1$Mgmt2016[df1$Mgmt2016==4]<-"OnFloodplain" # Formed on floodplain
df1$Mgmt2016[df1$Mgmt2016==5]<-"ManagedNamBCUC" #Managed by Manuna BCUC
df1$Mgmt2016[df1$Mgmt2016==6]<-"ManagedCommun" #managed by community
df1$Mgmt2016[df1$Mgmt2016==7]<-"ManagedPark" #managed by park

df1$Mgmt2022[df1$Mgmt2022==1]<-NA
df1$Mgmt2022[df1$Mgmt2022==2]<-"Burned"
df1$Mgmt2022[df1$Mgmt2022==3 | df1$Mgmt2022==6]<-"BurnedandMowed"
df1$Mgmt2022[df1$Mgmt2022==4|df1$Mgmt2022==5]<-"Mowed"
df1$Mgmt2022[df1$Mgmt2022==7]<-"MowedandUprooted"
df1$Mgmt2022[df1$Mgmt2022==8]<-"NoManagement"
df1$Mgmt2022[df1$Mgmt2022==9]<-"MowedUprootedBurned" #no sites in this category, kept for completeness


t1<-table(df1$Mgmt2022, df1$Location)

t1[,1]/sum(t1[,1]) #chitwan
t1[,2]/sum(t1[,2]) #parsa

df1$Mgmt2016<-as.factor(df1$Mgmt2016)
df1$Mgmt2022<-as.factor(df1$Mgmt2022)
df1$Location<-as.factor(df1$Location)
df1$fid <-as.factor(df1$fid)


#relevel
df1$Mgmt2022 <- relevel(factor(df1$Mgmt2022), "NoManagement")
df1$Mgmt2016 <- relevel(factor(df1$Mgmt2016), "Existing")
df1$Location <- relevel(factor(df1$Location), "Chitwan")



#### BOOTSTRAP TESTING ######################################
#loop through all samples, and run a test for each

s<-260 #representative sample (see below for derivation)
for(s in 1:max(df1$randomSeed)){
#for(s in 1:20){
  print(s)
  dfn<-df1[df1$randomSeed==s,]
  #dim(dfn)
  
  #take complete cases only
  dfn<-dfn[!is.na(dfn$Mgmt2022) & !is.na(dfn$Location),]
  names(dfn)
  
  dfn$fid<-factor(dfn$fid)
  
  #table(dfn$Mgmt2022, dfn$Location)
  glm1<-glmer(classification ~ Mgmt2022+Location+(1|fid), data=dfn, family=binomial(link="logit"),
              control=glmerControl("bobyqa"))

  #some model comparison code, left for reference
  # glm2<-glmer(classification ~ Mgmt2022*Location+(1|fid), data=dfn, family=binomial(link="logit"))
  #glm2<-glm(classification ~ Mgmt2022+Location, data=dfn, family=binomial(link="logit"))
  #
  # AIC(glm2, glm1)
  # anova(glm2, glm1)
  # logLik(glm2)
  # logLik(glm1)

  #summary(glm2)
  #no real difference in models, should favour parsimonious one?
  #table(dfn$classifica, dfn$Location)
  #table(dfn$classifica, dfn$Mgmt2022)
  
  
  ###
  #look at model outputs and diagnostics
  ###
  #summary(glm1)
  #mean(ranef(glm1)$fid[,1]) #random effect size around 0 -> good!
  
  # overdisp_fun <- function(model) {
  #   sum(residuals(model, type = "pearson")^2) / df.residual(model)
  # }
  # overdisp_fun(glm1)
  #no issues with overdispersion (though would be surprising with random effects)
  
  #https://cran.r-project.org/web/packages/DHARMa/vignettes/DHARMa.html
  #residuals are not great (in the QQ plot). But tough to correct without making a parameter for variance
  #after some searching (and trying different link functions), I think dredging for a "better" model would be a worse choice than just trusting bootstrapping
  #at this sample size, we bootstrap and converge as per central limit theorem
  #plot(glm1)
  
  # testDispersion(glm1)
  png(paste0("../diagnostics/diag",s,".png"), width=700, height=500)
  
  simulationOutput <- simulateResiduals(fittedModel = glm1, n=300, plot = T)

  dev.off()
  
  
  #suite one of model checks, overkill and left here for record
  #testResiduals(simulationOutput)
  #testDispersion(simulationOutput)
  #testZeroInflation(simulationOutput)
  
  #plotResiduals(simulationOutput, form = dfn$Mgmt2022)
  #testCategorical(simulationOutput, catPred = dfn$Mgmt2022)
  #many models have some issues with non-homogeneity of variance
  #however the actual size differences are small, this could be an effect of sample size

  #even more model checks, even more overkill
  
  # check normal distribution of random intercepts
  # qqnorm(ranef(glm1)$fid[,1], main="qq-plot, farm")
  # qqline(ranef(glm1)$fid[,1])
  # 
  # # residuals vs fitted values to check homoscedasticity
  # plot(fitted(glm1), resid(glm1)) 
  # abline(h=0)
  
  #ok actually look at model output
  summary(glm1)
  
  #extract parameters and save
  cest<-glm1@beta
  randi<-glm1@theta
  pvals<-coef(summary(glm1))[,4]
  cnames<-row.names(coef(summary(glm1)))
  
  #save output as temp file, compile later
  outdf<-data.frame("run"=s, "nrow"=nrow(dfn),"para"=cnames, "est"=cest, "pvals"=pvals, "randSD"=randi)
  write.csv(outdf, paste0("../modelOutput/m", s, ".csv"), row.names=F)
  
  
  #can plot if you want
  dfn$pred <- predict(glm1, type="response")
  p1<-ggplot(dfn,aes(x=Mgmt2022,y=pred,colour=fid, group=fid)) + 
    geom_jitter(position = position_jitter(0.2), alpha=0.1) + #geom_line()+
    xlab("Management")+
    ylab("Probability of Mikania presence")+
    theme(legend.position="none", legend.direction = "horizontal")
  
  p2<-ggplot(dfn, aes(x=Mgmt2022, y=pred)) + 
    geom_boxplot(outlier.colour="red", outlier.shape=8,
                 outlier.size=2)+
    xlab("Management")+
    ylab("")
    #geom_jitter(shape=16, position=position_jitter(0.2), alpha=0.1)
  
  p3<-ggplot(dfn,aes(x=Location,y=pred,colour=fid, group=fid)) + 
    geom_jitter(position = position_jitter(0.2), alpha=0.1) + #geom_line()+
    xlab("Location")+
    ylab("Probability of Mikania presence")+
    theme(legend.position="none", legend.direction = "horizontal")
  
  p4<-ggplot(dfn, aes(x=Location, y=pred)) + 
    geom_boxplot(outlier.colour="red", outlier.shape=8,
                 outlier.size=2)+
    xlab("Location")+
    ylab("")
  #geom_jitter(shape=16, position=position_jitter(0.2), alpha=0.1)
  #x11(width=10, height=7)
  gar<-ggarrange(p1,p2,p3,p4, ncol = 2, nrow = 2)
  ggsave(paste0("../modelOutput/m", s, ".png"), gar, width=10, height=7)
  #dev.off()
}


#### NEATEN AND SUMMARISE OUTPUT. THEN PLOT ######################################
### summarise allthe output


#the following is for dummy data
# 
# #pretend we have all the data
# row.names(outdf)<-NULL
# 
# 
# #make a plot
# #rep(outdf, 2)
# outdup<-cbind(outdf, 2, rep(row.names(outdf), each = 25))
# outdup$`rep(row.names(outdf), each = 25)`<-NULL
# outdup$run<-rep(1:25, each =6)
# 
# outdup[7:nrow(outdup),"est"]<-outdup[7:nrow(outdup),"est"]+rnorm(nrow(outdup)-6, 0, 2)
# outdup[7:nrow(outdup),"pvals"]<-outdup[7:nrow(outdup),"pvals"]+rnorm(nrow(outdup)-6, 0, 0.1)
# outdup[7:nrow(outdup),"randSD"]<-outdup[7:nrow(outdup),"randSD"]+rnorm(nrow(outdup)-6, 0, 0.2)
# 

#get all model output and compile into single dataset
outf<-list.files("../modelOutput/", ".csv")
outdup<-read.csv(paste0("../modelOutput/",outf[1]))

for (q in 2:length(outf)){
  outn<-read.csv(paste0("../modelOutput/",outf[q]))
  outdup<-rbind(outdup, outn)
}

#reorder by run
outdup<-outdup[order(outdup$run),]


#rename parameters for plotting
outdup$para[outdup$para=="(Intercept)"]<-"Intercept"
outdup$para[outdup$para=="Mgmt2022Burned"]<-"Burning"
outdup$para[outdup$para=="Mgmt2022BurnedandMowed"]<-"Burning and Mowing"
outdup$para[outdup$para=="Mgmt2022Mowed"]<-"Mowing"
outdup$para[outdup$para=="Mgmt2022MowedandUprooted"]<-"Mowing and Uprooting"
outdup$para[outdup$para=="LocationParsa"]<-"Parsa"

#let's plot the rolling parameter average to check for convergence
#Calculate moving average with window 3 and make first and last value as NA (to ensure identical length of vectors)
m.int<-cummean(outdup$est[outdup$para=="Intercept"])
m.var1<-cummean(outdup$est[outdup$para=="Burning"])
m.var2<-cummean(outdup$est[outdup$para=="Burning and Mowing"])
m.var3<-cummean(outdup$est[outdup$para=="Mowing"])
m.var4<-cummean(outdup$est[outdup$para=="Mowing and Uprooting"])
m.var5<-cummean(outdup$est[outdup$para=="Parsa"])

#also useful to check all parameter estimates are normal
png("../diagnostics/ParEst.png", height=480, width=400)
par(mfrow=c(3,2))
for(q in unique(outdup$para)){
  n1<-outdup$est[outdup$para==q]
  hist(n1,breaks=20, 
       main=paste0(q,": ", round(mean(n1),2), " (sd=", round(sd(n1),2), ")"))
}
dev.off()


#quick check of p values (not that we should trust them)
pv<-outdup[outdup$para=="Parsa","pvals"]
hist(pv)
median(pv)

#check random effect as well
length(unique(outdup$randSD))
m.rand <- cummean(outdup[match(unique(outdup$run), outdup$run),"randSD"])
ggplot(data=NULL,aes(1:length(m.rand), m.rand)) + geom_line() + 
  geom_line(aes(1:length(m.rand),m.rand),color="red") + 
  xlab("run") + ylab("est")+
  ggtitle("Intercept")
mean(m.rand)
quantile(m.rand, c(0.025, 0.975))
ggsave("../diagnostics/RandomID_convergence.png", width=10,height=8)


#random effects: remember the standard deviation implies how far the parameters vary by ID
#−1.96σ to +1.96σ
-1.96*mean(m.rand)
+1.96*mean(m.rand)
#in this case 2.8 total variance around the parameter estimate
#for example in our model the effect of mowed and uprooted is 4.12
#so estimate with RE is 
#4.1200-1.96*mean(m.rand)
# 4.1200+1.96*mean(m.rand)
#which when converted back makes
#intercept+effect size +/0 random effect deviance
#this gives 95% estimates of real data
inv.logit(-0.88+0)
inv.logit(-0.88+0-1.96*mean(m.rand))
inv.logit(-0.88+0+1.96*mean(m.rand))

inv.logit(-0.88+0)
inv.logit(-0.88+5.58-1.96*mean(m.rand))
inv.logit(-0.88+5.58+1.96*mean(m.rand))

#0.37-0.96-0.99
#substantial variance



#Add additional line for moving average in red
p<-ggplot(data=NULL,aes(1:length(m.int), m.int)) + geom_line() + 
  geom_line(aes(1:length(m.int),m.int),color="red") + 
  xlab("run") + ylab("est")+
  ggtitle("Intercept")
p2<-ggplot(data=NULL,aes(1:length(m.var1), m.var1)) + geom_line() + 
  geom_line(aes(1:length(m.var1),m.var1),color="red") + 
  xlab("run") + ylab("est")+
  ggtitle("Burning")
p3<-ggplot(data=NULL,aes(1:length(m.var2), m.var2)) + geom_line() + 
  geom_line(aes(1:length(m.var2),m.var2),color="red") + 
  xlab("run") + ylab("est")+
  ggtitle("Burning and Mowing")
p4<-ggplot(data=NULL,aes(1:length(m.var3), m.var3)) + geom_line() + 
  geom_line(aes(1:length(m.var3),m.var3),color="red") + 
  xlab("run") + ylab("est")+
  ggtitle("Mowing")
p5<-ggplot(data=NULL,aes(1:length(m.var4), m.var4)) + geom_line() + 
  geom_line(aes(1:length(m.var4),m.var4),color="red") + 
  xlab("run") + ylab("est")+
  ggtitle("Mowing and Uprooting")
p6<-ggplot(data=NULL,aes(1:length(m.var5), m.var5)) + geom_line() + 
  geom_line(aes(1:length(m.var5),m.var5),color="red") + 
  xlab("run") + ylab("est")+
  ggtitle("Parsa")

library(ggpubr)
gar<-ggarrange(p,p2,p3,p4,p5,p6, ncol = 3, nrow = 2)
ggsave(paste0("../diagnostics/RunConvergence.png"), gar)

#some issues with Parsa convergence, but otherwise pretty good

#outdup is now our output to work with
outdup$para<-factor(outdup$para, levels=c("Intercept", "Burning", "Mowing", "Burning and Mowing", "Mowing and Uprooting", "Parsa"))
#boxplot(outdup$est~outdup$para)

ggplot(outdup, aes(x=para, y=est, fill=para)) + 
  geom_boxplot(outlier.colour="red", outlier.shape=8,
               outlier.size=2)+
  theme_bw()+
  theme(legend.position="none")


#take the mean and 97.5 and 2.5 percentile notes
r1<-c(mean(outdup$est[outdup$para=="Intercept"]),quantile(outdup$est[outdup$para=="Intercept"], c(0.025, 0.975)))
names(r1)[1]<-"mean"; r1<-as.data.frame(t(r1))
r1$parameter<-"Intercept"; 

r2<-c(mean(outdup$est[outdup$para=="Burning"]),quantile(outdup$est[outdup$para=="Burning"], c(0.025, 0.975)))
names(r2)[1]<-"mean"; r2<-as.data.frame(t(r2))
r2$parameter<-"Burning"; 

r3<-c(mean(outdup$est[outdup$para=="Burning and Mowing"]),quantile(outdup$est[outdup$para=="Burning and Mowing"], c(0.025, 0.975)))
names(r3)[1]<-"mean"; r3<-as.data.frame(t(r3))
r3$parameter<-"Burning and Mowing"; 

r4<-c(mean(outdup$est[outdup$para=="Mowing"]),quantile(outdup$est[outdup$para=="Mowing"], c(0.025, 0.975)))
names(r4)[1]<-"mean"; r4<-as.data.frame(t(r4))
r4$parameter<-"Mowing"; 

r5<-c(mean(outdup$est[outdup$para=="Mowing and Uprooting"]),quantile(outdup$est[outdup$para=="Mowing and Uprooting"], c(0.025, 0.975)))
names(r5)[1]<-"mean";  r5<-as.data.frame(t(r5))
r5$parameter<-"Mowing and Uprooting";

r6<-c(mean(outdup$est[outdup$para=="Parsa"]),quantile(outdup$est[outdup$para=="Parsa"], c(0.025, 0.975)))
names(r6)[1]<-"mean"; r6<-as.data.frame(t(r6))
r6$parameter<-"Parsa";

rall<-rbind(r1,r2,r3,r4,r5,r6)
rall
inv.logit(rall$mean)

colnames(rall)<-c("mean", "i250", "i975", "parameter")

rall$parameter<-factor(rall$parameter, levels=c("Intercept", "Burning", "Mowing", "Burning and Mowing", "Mowing and Uprooting", "Parsa"))




ggplot(data=rall[rall$parameter!="Parsa",], aes(x=parameter, y=mean, colour=parameter)) + geom_point(aes(size=4))+
  geom_errorbar(aes(ymin = i250, ymax = i975), width=0.5)+  
  theme_bw()+
  xlab("Management Approach")+
  ylab("Parameter Estimate")+
  theme(legend.position="none")
ggsave("../ParRawFig.png", width=7, height=5)

#rall is the source of data for Table 3 in the submitted paper
rall


rtran<-data.frame(parameter=rall$parameter, mean=NA, i25=NA, i95=NA)
rtran[1,2:4]<-inv.logit(as.numeric(rall[rall$parameter=="Intercept", 1:3]))

inv.logit(c(-0.88, -1.14, -0.64))

i<-2
st<-rall[rall$parameter=="Intercept", "mean"]
for(i in 2:nrow(rall)){
  nowp<-rtran[i,"parameter"]
  nowm<-inv.logit(st+rall[rall$parameter==nowp,"mean"])
  now25<-inv.logit(st+rall[rall$parameter==nowp,"i250"])
  now95<-inv.logit(st+rall[rall$parameter==nowp,"i975"])
  
  rtran[i,2:4]<-c(nowm, now25, now95)
}


# ggplot(data=rall, aes(x=parameter, y=mean, colour=parameter)) + geom_point(aes(size=4))+
#   geom_errorbar(aes(ymin = i250., ymax = i975), width=0.5)+  
#   theme_bw()+
#   theme(legend.position="none")
# ggsave("../ParRawFig.png")

ggplot(data=rtran[rtran$parameter!="Parsa",], aes(x=parameter, y=mean, colour=parameter)) + geom_point(aes(size=4))+
  geom_errorbar(aes(ymin = i25, ymax = i95), width=0.5)+  
  theme_bw()+  
  xlab("Management Approach")+
  ylab("Parameter Estimate")+
  theme(legend.position="none")
ggsave("../ParTransFig.png", width=7, height=5)



# theme_bw()# for random effects look at standard deviance for random effects look at standard deviation. The STD variance of intercepts is 1.931 ()
#inv.logit(1.931) #around 0.87 variance across IDs

#optionally get confidence intervals
confint(glm1)


### Final part: find a representative model
#use as illustrative example in manuscript
#we want a model close to median parameter estimates
outfind<-outdup

outfind<-merge(outdup, rall, by.x="para", by.y="parameter")
outfind$diff<-abs(outfind$mean-outfind$est)
outc<-aggregate(diff ~ run, outfind, sum)

outc[which.min(abs(outc$diff)),]
outfind[outfind$run==260,]

#260 is very close to overall average.


